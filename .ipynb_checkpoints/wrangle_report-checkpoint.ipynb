{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7e8c55f",
   "metadata": {},
   "source": [
    "### Reporting: Wrangle Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449387a2",
   "metadata": {},
   "source": [
    "My wrangling efforts were divided into three categories. First, I had to gather the data, next assess it, and then clean it.\n",
    "\n",
    ">During the gathering phase, I had three different sources to pull data from. \n",
    "\n",
    "At first, I was handed a data file containing the archived tweet information for the userâ€™s account. Next, I had to programmatically download a TSV file containing image prediction information already processed by a neural network.\n",
    "\n",
    "Finally, I queried the Twitter API using the access library - tweepy for the tweet IDs in the archive file.\n",
    "To achieve this, I signed up for a Twitter developer account and generated my API and Access keys and tokens respectively.\n",
    "With my access, I scraped data off the API and stored it in a txt file after converting the status for each tweet ID to JSON format.\n",
    "I later proceeded to read the data from the txt file line by line and extract the data points I wanted.\n",
    "For all the data I gathered, I created a data frame to store each accordingly.\n",
    "\n",
    "\n",
    ">Once I had gathered the data, I had to assess them for errors, completeness issues, validity, accuracy, and consistency problems.\n",
    "\n",
    "I made both visual and programmatic assessments of the different data frames.\n",
    "During the visual assessment, I scrolled through the data frame after opening it with pandas, to view any anomaly or irregularity in the data.\n",
    "I also open it up in MS Excel to have a wider look into the data frame.\n",
    "\n",
    "Whereas I simply used code to observe patterns in the data set and query inconsistencies.\n",
    "For every quality that made the data either dirty or untidy, I documented it accordingly.\n",
    "After assessment and documentation, it was time to clean the data and prepare it for analysis.\n",
    "\n",
    ">Before cleaning could begin I made a copy of the original data frames so I could retain the files before making alterations.\n",
    "\n",
    "In the Cleaning phase, I used programmatic means to drop some rows and columns, merge the data sets, replace misleading data points, correct data types, and reorganize the structure of the tables.\n",
    "\n",
    "When the cleaning was done, I created a master data set by merging the requisite columns from all the tables I had cleaned.\n",
    "I then saved the data to a CSV file in my working directory. I also saved the individual data frames for tidiness reasons.\n",
    "\n",
    "In the end, I generated some useful insights from the data and created some visualizations to communicate them too.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
